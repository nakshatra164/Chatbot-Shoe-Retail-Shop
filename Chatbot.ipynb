{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f013fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in c:\\users\\nakshatra\\desktop\\chatbot\\env\\lib\\site-packages (0.5.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\nakshatra\\appdata\\roaming\\python\\python310\\site-packages (from tflearn) (1.23.1)\n",
      "Requirement already satisfied: six in c:\\users\\nakshatra\\appdata\\roaming\\python\\python310\\site-packages (from tflearn) (1.16.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nakshatra\\appdata\\roaming\\python\\python310\\site-packages (from tflearn) (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edf31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nakshatra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nakshatra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Nakshatra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd92bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nakshatra\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Importing Libraries needed for Tensorflow processing\n",
    "import tensorflow as tf   #version 1.13.2\n",
    "import numpy as np\n",
    "import tflearn            #version 0.3.2\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889da0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our intent file used for training the model.\n",
    "with open(\"intents.json\") as json_data: \n",
    "    intents = json.load(json_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab4c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for appending the data after processing NLP\n",
    "words=[]\n",
    "documents = []\n",
    "classes = []\n",
    "\n",
    "\n",
    "# This list will be used for ignoring all unwanted punctuation marks.\n",
    "ignore = [\"?\",'.',',','!']\n",
    "\n",
    "# Starting a loop through each intent in intents[\"patterns\"]\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        \n",
    "        # tokenizing each and every word in the sentence by using word tokenizer and storing in w\n",
    "        w = nltk.word_tokenize(pattern) \n",
    "        #print(w)\n",
    "        \n",
    "        # Adding tokenized words to words empty list that we created\n",
    "        words.extend(w) \n",
    "        #print(words)\n",
    "        \n",
    "        # Adding words to documents with tag given in intents file\n",
    "        documents.append((w, intent[\"tag\"]))\n",
    "        #print(documents)\n",
    "        \n",
    "        # Adding only tag to our classes list\n",
    "        if intent[\"tag\"] not in classes:      \n",
    "            classes.append(intent[\"tag\"])  #If tag is not present in classes[] then it will append into it.\n",
    "            #print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4125ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 Documents \n",
      "\n",
      "17 Classes \n",
      "\n",
      "65 Stemmed Words \n"
     ]
    }
   ],
   "source": [
    "#Performing Stemming by using stemmer.stem() nd lower each word \n",
    "#Running loop in words[] and ignoring punctuation marks present in ignore[]\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
    "words = sorted(list(set(words)))  #Removing Duplicates in words[]\n",
    "\n",
    "#Removing Duplicate Classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "#Printing length of lists we formed\n",
    "print(len(documents),\"Documents \\n\")\n",
    "print(len(classes),\"Classes \\n\")\n",
    "print(len(words), \"Stemmed Words \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ef78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Training Data which will be furthur used for training\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "#Creating empty array for output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "#Creating Training set and bag of words for each sentence\n",
    "for doc in documents:\n",
    "    bag = [] #Initialising empty bag of words\n",
    "\n",
    "    pattern_words = doc[0] #Storing list of tokenized words for the documents[] tp pattern_words\n",
    "    #print(pattern_words)\n",
    "    \n",
    "    #Again Stemming each word from pattern_words\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
    "    #print(pattern_words)\n",
    "    \n",
    "    #Creating bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    #It will give output 1 for curent tag and 0 for all other tags\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] =1 \n",
    "    training.append([bag, output_row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8745d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nakshatra\\AppData\\Local\\Temp\\ipykernel_14304\\1336731122.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training) #Converting training data into numpy array\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training) #Suffling the data or features\n",
    "training = np.array(training) #Converting training data into numpy array\n",
    "\n",
    "#Creating Training Lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12999  | total loss: \u001b[1m\u001b[32m0.00153\u001b[0m\u001b[0m | time: 0.024s\n",
      "| Adam | epoch: 1000 | loss: 0.00153 - acc: 1.0000 -- iter: 096/104\n",
      "Training Step: 13000  | total loss: \u001b[1m\u001b[32m0.00154\u001b[0m\u001b[0m | time: 0.024s\n",
      "| Adam | epoch: 1000 | loss: 0.00154 - acc: 1.0000 -- iter: 104/104\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Nakshatra\\Desktop\\chatbot\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "#ops.reset_default_graph()\n",
    "\n",
    "ops.reset_default_graph() #Reset Underlying Graph data\n",
    "\n",
    "#Building our own Neural Network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "#Defining Model and setting up tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir=\"tflearn_logs\") \n",
    "\n",
    "#Now we have setup model, now we need to train that model by fitting data into it by model.fit()\n",
    "#n_epoch is the number of times that model will se our data during training\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True) \n",
    "model.save(\"model.tflearn\") #Saving the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de5227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pickle module\n",
    "import pickle\n",
    "\n",
    "#Dumping training data by using dump() and writing it into training_data in binary mode\n",
    "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "846bb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restoring all data structure\n",
    "data = pickle.load(open(\"training_data\",\"rb\"))\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cdd3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\") as json_data:\n",
    "    intents = json.load(json_data)  #Loading our json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfebe3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Nakshatra\\Desktop\\chatbot\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved model\n",
    "model.load(\"./model.tflearn\") #Loading training model which we saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7c02260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning User Input\n",
    "def clean_up_sentence(sentence):\n",
    "    \n",
    "    # Tokenizing the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence) #Again tokenizing the sentence\n",
    "    \n",
    "    #Stemming each word from the user's input\n",
    "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "#Returning bag of words array: 0 or 1 or each word in the bag that exists in as we have declared in above lines\n",
    "def bow(sentence, words, show_details=False):\n",
    "    \n",
    "    #Tokenizing the user input\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    #Generating bag of words from the sentence that user entered\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"Found in bag: %s\"% w)\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db6a6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding some context to the conversation for better results.\n",
    "\n",
    "context = {} #Create a dictionary to hold user's context\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify(sentence):\n",
    "    \n",
    "    #Generating probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    \n",
    "    #Filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    \n",
    "    #Sorting by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    \n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    \n",
    "    #If we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        \n",
    "        #Loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                \n",
    "                #Find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    \n",
    "                    #Set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        \n",
    "                        #A random response from the intent\n",
    "                        return random.choice(i['responses'])\n",
    "\n",
    "            results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7407ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go..Bot is Running!!!\n",
      "Hello\n",
      "Hello.. Welcome to Shoe Tex. We have a variety of shoes available.May I know Your Preferences\n",
      "tell me more\n",
      "We offer collections of \n",
      "1.Sneakers\n",
      "2.Boots\n",
      "3.Slippers\n",
      "4.Sports.\n",
      "Enter your preference to get more details.\n",
      "Sports\n",
      "Flexible and Comfortable styles for Sports Shoes\n",
      "1.High Ankle Knitwear Running Shoes for Men\n",
      "2.Men's Wonder-13 Sports Running Shoes\n",
      "3.Men's Rugged Trekking Shoes 1225\n",
      "4.Women Walking Shoes\n",
      "5.Slip on Running Sports Shoes\n",
      "If you want to filter search for men/women Type\n",
      "* Sport shoes for Men - for men's Slippers\n",
      "* Sports shoes for Women - for women's Slippers\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Go..Bot is Running!!!\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message==\"exit\":\n",
    "        break\n",
    "    ints = classify(message)\n",
    "    res = response(message,intents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23e66f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "bot_name = \"ChatBot\"\n",
    "import jarvis_edit as js\n",
    "\n",
    "BG_GRAY = \"#FFD700\"\n",
    "BG_COLOR = \"#FFFFCB\"\n",
    "TEXT_COLOR = \"#000000\"\n",
    "SEND_COLOR = '#F0FFFF'\n",
    "\n",
    "FONT = \"Helvetica 14\"\n",
    "FONT_BOLD = \"Helvetica 13 bold\"\n",
    "\n",
    "\n",
    "class ChatApplication:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.window = Tk()\n",
    "        self._setup_main_window()\n",
    "        \n",
    "    def run(self):\n",
    "        self.window.mainloop()\n",
    "        \n",
    "    def _setup_main_window(self):\n",
    "        self.window.title(\"Shoe Tex ChatBot\")\n",
    "        self.window.resizable(width = True,height=True)\n",
    "        self.window.configure(width=600,height=700,bg= BG_COLOR)\n",
    "        \n",
    "        #head label\n",
    "        head_label = Label(self.window,bg=BG_GRAY,fg=TEXT_COLOR,\n",
    "                          text=\"WELCOME...\",font = FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "        \n",
    "        #tiny dicider\n",
    "        line = Label(self.window,width = 450,bg = BG_COLOR)\n",
    "        line.place(relwidth=1,rely=0.07,relheight=0.012)\n",
    "        \n",
    "        #text widget\n",
    "        self.text_widget = Text(self.window,width=20,height=2,bg=TEXT_COLOR,fg = SEND_COLOR,font=FONT,padx=5,pady=5)\n",
    "        self.text_widget.place(relheight=0.745,relwidth=1,rely=0.08)\n",
    "        self.text_widget.configure(cursor=\"arrow\",state=DISABLED)\n",
    "        \n",
    "        #scrollbar\n",
    "        scrollbar = Scrollbar(self.text_widget)\n",
    "        scrollbar.place(relheight=1,relx=0.975)\n",
    "        scrollbar.configure(command=self.text_widget.yview)\n",
    "        \n",
    "        #bottom label\n",
    "        bottom_label = Label(self.window,bg = BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "        \n",
    "        #text Entry widget\n",
    "        self.msg_entry = Entry(bottom_label,bg=BG_COLOR,fg=TEXT_COLOR,font=FONT)\n",
    "        self.msg_entry.place(relwidth=0.74,relheight=0.06,rely=0.008,relx=0.011)\n",
    "        self.msg_entry.focus()\n",
    "        self.msg_entry.bind(\"<Return>\",self._on_enter_pressed)\n",
    "    \n",
    "        #send button\n",
    "        send_button = Button(bottom_label,text=\"Send\",font=FONT_BOLD,width=20,bg=BG_GRAY,\n",
    "                        command=lambda:self._on_enter_pressed(None))\n",
    "        send_button.place(relx=0.77,rely=0.008,relheight=0.06,relwidth=0.22)\n",
    "        \n",
    "    def _on_enter_pressed(self,event):\n",
    "        msg = self.msg_entry.get()\n",
    "        self._insert_message(msg,\"You:\")\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def _insert_message(self,msg,sender):\n",
    "        if not msg:\n",
    "            return\n",
    "        self.msg_entry.delete(0,END)\n",
    "        #js.takeCommand()\n",
    "        msg1 = f\"{sender}: {msg}\\n\\n\"\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        self.text_widget.insert(END,msg1)\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        res = response(msg,intents)\n",
    "        \n",
    "        msg2 = f\"{bot_name}: {res}\\n\\n\"\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        self.text_widget.insert(END,msg2)\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        \n",
    "        self.text_widget.see(END)\n",
    "        js.speak(res)\n",
    "        \n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    js.wish_me()\n",
    "    app = ChatApplication()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf81c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfcbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
